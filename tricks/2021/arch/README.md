# 从软件的历史看架构的未来

:::quote 软件架构与硬件算力规模对齐

As long as there were no machines, programming was no problem at all; when we had a few weak computers, programming became a mild problem, and now we have gigantic computers, programming has become an equally gigantic problem.

在没有计算机的时候，也就没有编程问题；当我们有了简单的计算机，编程只是个小问题；而现在我们有了算力规模庞大的计算机，那编程就成为了一个同样巨大的问题了。

:::right

—— [Edsger Dijkstra](https://en.wikipedia.org/wiki/Edsger_W._Dijkstra), [Communications of the ACM](https://en.wikipedia.org/wiki/Communications_of_the_ACM), 1972

:::

1972年，[Edsger Dijkstra](https://en.wikipedia.org/wiki/Edsger_W._Dijkstra)在图灵奖颁授现场发表获奖感言时说了这么一段话：“在没有计算机的时候，也就没有编程问题；当我们有了简单的计算机，编程只是个小问题；而现在我们有了算力规模庞大的计算机，那编程也就成为了一个同样巨大的问题了”。每当人类掌握了更强的计算能力，总会按耐不住要拿去解决一些以前甚至都不敢设想的新问题，而如何把充分硬件的能力发挥出来，那就是软件责任，这也是每个时代软件架构变革的根本动力。

## 历史上的软件危机和契机

在计算机刚诞生不久的年代，硬件的算力规模还很小，甚至程序员的大脑足够记住数据在几KB内存中的分布细节，记住每项操作在电路中的运行逻辑，此时的计算机尽管运算速度比人类快，但并内部没有什么事情人所不知道的。此时的软件开发并没有独立的“架构”可言，软件的架构与硬件的架构是直接物理对齐的。

随着机器的发展，直接面向硬件架构的软件开发很快触碰到瓶颈，人脑的生物局限显然无法跟上机器算力前进的步伐，这便是历史上的第一次软件危机的根源——世界上最聪明的人都无法为机器编写出合理的程序了。第一次软件危机也是高级语言和结构化编程出现的契机，高级语言屏蔽大多数硬件细节，获得了相对良好的抽象能力与可移植性，结构化编程强化了可独立编写、可重复利用的子程序的思想，让软件的每个局部都可以拥有独立的算法和数据结构，从而在整体上控制住了软件开发的复杂度，允许每一位程序员只关注自己所负责的部分。直至此时，软件的架构才开始宣告独立，软件业出现了架构师（控制全局）与程序员（关注局部）的角色分工。

把软件从一个整体拆分为多个局部个体，让人类能够以群体配合来共同开发一个软件，又让人类与计算机和谐共处了十余年，但是当算力规模继续膨胀，人类群体的沟通能力也碰到了极限，这是第二次软件危机。人毕竟不是可复制的程序，每个人都有自己的认知，每个模块都有自己的程序结构，有自己的算法，有自己的数据结构，如何让每个模块都能准确的协同工作就成了一场灾难。《人月神话》那个几乎每位程序员都读过的案例：IBM公司开发的OS/360系统共有4000多个模块，约100万条指令，投入5000人年，耗资数亿美元，结果还是延期交付。在交付使用后的系统中仍发现大量的错误。

:::quote An Introduction to Software Architecture

As the size of software systems increases, the algorithms and data structures of the computation no longer constitute the major design problems. When systems are constructed from many components, the organization of the overall system-the software architecture-presents a new set of design problems.

随着软件系统规模的增加，计算相关的算法和数据结构不再构成主要的设计问题；当系统由许多部分组成时，整个系统的组织，也就是所说的“软件架构”，导致了一系列新的设计问题。

:::right

——David Garlan and Mary Shaw, [An Introduction to Software Architecture](https://userweb.cs.txstate.edu/~rp31/papers/intro_softarch.pdf), 1994

:::

为了解决这次危机，诞生了面向对象的编程语言（C++、C#、Java等），也诞生了更好的软件工程理论与实践方法（设计模式、重构、测试、需求分析等等），而程序员们也越来越不需要知道硬件是怎么工作的了。软件和硬件的界限越来越牢固，Java编写的代码能在任何JVM支持的平台上运行，程序员也非常乐于享受这样的便利。

## 云与分布式渐成为主流

如果说历史上的第一、二次软件分别是机器算力规模超过了人类个体的生理极限，超过了人类群体的沟通极限的话。那在今天，在云计算时代，数据中心所能提供的算力已经逼近了人类协作的工程极限，与算力相符合的软件最大规模，几乎达到了无论如何优化工程措施，无论采用什么管理手段提升质量，都肯定会有人疏忽犯错，会有代码携带缺陷，会有电脑宕机崩溃，会有网络堵塞中断……如果一项工程需要大量的人员，共同去研发某个大规模的软件产品，并使其分布在网络中大量的服务器节点中同时运行，随着项目规模的增大、运作时间变长，其必然会受到墨菲定律的无情打击。

软件架构风格经历了大型机（Mainframe），到[原始分布式](http://192.168.30.2:8080/architecture/architect-history/primitive-distribution.html)（Distributed），到[大型单体](http://192.168.30.2:8080/architecture/architect-history/monolithic.html)（Monolithic），到[面向服务](http://192.168.30.2:8080/architecture/architect-history/soa.html)（Service-Oriented），到[微服务](http://192.168.30.2:8080/architecture/architect-history/microservices.html)（Microservices），到[服务网格](http://192.168.30.2:8080/architecture/architect-history/post-microservices.html)（Service Mesh），到[无服务](http://192.168.30.2:8080/architecture/architect-history/serverless.html)（Serverless）

……技术架构上确实呈现出“从大到小”的发展趋势。当近年来微服务兴起以后，涌现出各类文章去总结、赞美微服务带来的种种好处，诸如简化部署、逻辑拆分更清晰、便于技术异构、易于伸缩拓展应对更高的性能等等，这些当然都是重要优点和动力。可是，如果不拘泥于特定系统或特定某个问题，以更宏观的角度来看，前面所列这种种好处却都只能算是“锦上添花”、是属于让系统“活得更好”的动因，肯定比不上系统如何“确保生存”的需求来得关键、本质。笔者看来，架构演变最重要的驱动力，或者说这种“从大到小”趋势的最根本的驱动力，始终都是为了方便某个服务能够顺利地“死去”与“重生”而设计的，个体服务的生死更迭，是关系到整个系统能否可靠续存的关键因素。

举个例子，譬如某企业中应用的单体架构的 Java 系统，其更新、升级都必须要有固定的停机计划，必须在特定的时间窗口内才能按时开始，必须按时结束。如果出现了非计划的宕机，那便是生产事故。但是软件的缺陷不会遵循领导定下的停机计划来“安排时间出错”，为了应对缺陷与变化，做到不停机地检修，Java 曾经搞出了 OSGi 和 JVMTI Instrumentation 等这样复杂的 HotSwap 方案，以实现给奔跑中的汽车更换轮胎这种匪夷所思却又无可奈何的需求；而在微服务架构的视角下，所谓系统检修，不过只是一次在线服务更新而已，先停掉 1/3 的机器，升级新的软件版本，再有条不紊地导流、测试、做金丝雀发布，一切都是显得如此理所当然、平淡寻常；而在无服务架构的视角下，我们甚至都不可能去关心服务所运行的基础设施，连机器是哪台都不必知道，停机升级什么的就根本无从谈起了。

流水不腐，有老朽，有消亡，有重生，有更迭才是生态运行的合理规律。请设想一下，如果你的系统中每个部件都符合“Phoenix”的特性，哪怕其中某些部件采用了由极不靠谱的人员所开发的极不靠谱程序代码，哪怕存有严重的内存泄漏问题，最多只能服务三分钟就一定会崩溃。而即便这样，只要在整体架构设计有恰当的、自动化的错误熔断、服务淘汰和重建的机制，在系统外部来观察，整体上仍然有可能表现出稳定和健壮的服务能力。

## 未来：云不可知

我们不妨顺着这个思路，来探讨一下软件开发的下一个基本矛盾将会是什么？

面向错误设计的微服务，解决了

是算力规模超过人类可掌握的知识极限，

信息技术从来不缺乏[Buzzword](https://en.wikipedia.org/wiki/Buzzword)，近年来无限火热的云原生、微服务、不可变基础设施、弹性计算、服务网格、无服务器架构、高低零代码，等等，其背后往往都能展开成一整套成体系的开发或者架构方法。这些新的技术在为人们解决了更复杂软件问题的同时，也正在把“软件开发”这件事情本身推至更高层次的复杂度。



一名刚刚走出学校的本科生，到消化完几门编程语言的核心细节，再到掌握领域中常用的类库、框架和工具，再到接触云原生、分布式系统等架构层面的知识，平均而言，可能要耗费上8到10年的时间。8到10年已经是一段很漫长的人生了，哲学里，曾经严肃研讨过“知识膨胀”的问题，说的是人类科学的前沿在不断拓展，要触及到前沿所需的知识也需不断增加的话，是否会陷入后来者终其一生都无法攒下足够基础，而导致人类知识陷入止步不前的危机之中？

这个问题，我认为在科学界是不会发生的。我们学物理，直接接触的就是牛顿经典物理学，此前两千年来亚里士多德基于先验逻辑的物理学统治时间虽长，但已经被放到历史学而不是科学之中。科学是一簇一簇地在发展，新的一簇被认为更加合理的话，旧的一簇就会被科学共同体发一个牌位，到科学史上排排坐了。

DFX属性从软件开发中分离出来，让业务人员写业务代码，有专门的团队去维护DFX。